{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709cb8e-8df8-45b8-8aa3-30ff17752c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "from CartpoleAgent import FOLCartpoleAgent\n",
    "\n",
    "Observation = collections.namedtuple(\"Observation\", (\"Position\", \"Velocity\", \"Angle\", \"AngVelocity\"))\n",
    "Transition = collections.namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "WARMUP_EPISODES = 30\n",
    "NUM_EPISODES = 50\n",
    "\n",
    "n_bin_args = {\n",
    "    \"n_pos\": 2,\n",
    "    \"n_vel\": 2,\n",
    "    \"n_ang\": 2,\n",
    "    \"n_angvel\": 2\n",
    "}\n",
    "\n",
    "limits = {\n",
    "    \"Position\": [-1.2, 1.2],\n",
    "    \"Velocity\": [-2, 2],\n",
    "    \"Angle\": [-0.2094395, 0.2094395],\n",
    "    \"AngVelocity\": [-3, 3]\n",
    "}\n",
    "\n",
    "agent = FOLCartpoleAgent(n_bin_args, n_nodes = 5, limits = limits, t=\"double\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def train():\n",
    "    epsilon = EPSILON_START\n",
    "    episode_num = 0\n",
    "    episode_runs = []\n",
    "    episode_losses = []\n",
    "    for episode in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "        total_loss, step = 0,0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = agent.sample_random_action()\n",
    "            else:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            reward = reward/10 if not terminal else 0\n",
    "            agent.remember(Transition(state, action, next_state, reward, terminal))\n",
    "            loss = agent.optimize()\n",
    "            state = next_state\n",
    "            \n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "                step += 1\n",
    "            \n",
    "            if terminal or truncated:\n",
    "                if step > 0:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(step) + \", episode_loss: \" + str(total_loss/step))\n",
    "                    episode_runs.append(step)\n",
    "                    episode_losses.append(total_loss/step)\n",
    "                    epsilon -= (EPSILON_START - EPSILON_END)/WARMUP_EPISODES\n",
    "                    \n",
    "                    epsilon = min(epsilon, EPSILON_END)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48121f50-dbc3-4f8c-9bb0-628cfdd03860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd34ce9-0bd4-4536-89dd-7bdf8991188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c20ad-ed0c-4545-9a5a-3846d704bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "itertools.chain.from_iterable([n.parameters() for n in agent.lnn.model.nodes.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c5475-3789-449a-b8a6-0f2e8b45efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1b2f9-dbdc-462b-a50e-250bce2d68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in agent.lnn.model.nodes.values():\n",
    "    print(val.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "def distill_policy(self):\n",
    "    lnamed_params = self.left_lnn.model.named_parameters()\n",
    "    lpolicy = []\n",
    "    lor_weights = []\n",
    "    for key in lnamed_params:\n",
    "        if '∨' in key and '.weights' in key: #OR\n",
    "            lor_weights = lnamed_params[key]\n",
    "        elif '∧' in key and '.weights' in key:\n",
    "            weights = lnamed_params[key]\n",
    "            used_inputs = weights > (1./len(weights))\n",
    "            input_names = key[1:-(len(').weights'))].split(' ∧ ')\n",
    "            used_inputs = [inp[:-3] for i, inp in enumerate(input_names) if used_inputs[i]]\n",
    "            rule = ''\n",
    "            if used_inputs:\n",
    "                rule = ' ∧ '.join(used_inputs)\n",
    "            lpolicy.append(rule)\n",
    "\n",
    "    rnamed_params = self.right_lnn.model.named_parameters()\n",
    "    rpolicy = []\n",
    "    ror_weights = []\n",
    "    for key in rnamed_params:\n",
    "        if '∨' in key and '.weights' in key: #OR\n",
    "            ror_weights = rnamed_params[key]\n",
    "        elif '∧' in key and '.weights' in key:\n",
    "            weights = rnamed_params[key]\n",
    "            used_inputs = weights > (1./len(weights))\n",
    "            input_names = key[1:-(len(').weights'))].split(' ∧ ')\n",
    "            used_inputs = [inp[:-3] for i, inp in enumerate(input_names) if used_inputs[i]]\n",
    "            rule = ''\n",
    "            if used_inputs:\n",
    "                rule = ' ∧ '.join(used_inputs)\n",
    "            rpolicy.append(rule)\n",
    "    return lpolicy, lor_weights, rpolicy, ror_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c74c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpolicy, lor_weights, rpolicy, ror_weights = distill_policy(agent)\n",
    "for rule in lpolicy:\n",
    "    print(rule)\n",
    "print(lor_weights)\n",
    "for rule in rpolicy:\n",
    "    print(rule)\n",
    "print(ror_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b6c72-d519-46fc-9c0d-976441a89b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = agent.left_lnn.model.parameters_grouped_by_neuron()\n",
    "named_params = agent.left_lnn.model.named_parameters()\n",
    "used = []\n",
    "used_ands = torch.Tensor()\n",
    "for x in parameters:\n",
    "    if x['neuron_type'] == 'And':\n",
    "        weights = x['params'][2]\n",
    "        used.append(weights > (1./len(weights)))\n",
    "    elif x['neuron_type'] == 'Or':\n",
    "        or_weights = parameters[-1]['params'][2]\n",
    "        used_ands = or_weights > (1./len(or_weights))\n",
    "for u in used:\n",
    "    print(u)\n",
    "print(used_ands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fd742-39e1-4ca0-874b-bc519d363683",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.left_lnn.model.print(params = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868a2f9-5c5e-456f-ae68-e6570c8ac106",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.lnn.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22344a-0382-4cb3-8f3a-b88654daf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "action = agent.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6646cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill2(self):\n",
    "    '''\n",
    "    returns:\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill3(self):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = distill(agent)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eef988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import tqdm\n",
    "\n",
    "from CartPole import DQNSolver\n",
    "\n",
    "Observation = collections.namedtuple('Observation', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "dqn_solver = DQNSolver(observation_space, action_space)\n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.97\n",
    "epsilon = epsilon_start\n",
    "\n",
    "max_episodes = 150\n",
    "episode = 0\n",
    "\n",
    "episode_runs = []\n",
    "episode_losses = []\n",
    "while episode < max_episodes:\n",
    "    episode += 1\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(np.reshape(state[0], [1, observation_space]))\n",
    "    step = 0\n",
    "    episode_loss = 0\n",
    "    while True:\n",
    "        action = dqn_solver.choose_action(state,epsilon)\n",
    "        state_next, reward, terminal, truncated, info = env.step(action)\n",
    "        reward = reward if not terminal else 0\n",
    "        state_next = torch.tensor(np.reshape(state_next, [1, observation_space]))\n",
    "        dqn_solver.remember(Observation(state, action, reward, state_next, terminal))\n",
    "        loss = dqn_solver.replay()\n",
    "\n",
    "        if loss is not None:\n",
    "            episode_loss += loss\n",
    "            state = state_next\n",
    "            step += 1\n",
    "\n",
    "        if terminal or truncated:\n",
    "            if step > 0:\n",
    "                print(\"Run: \" + str(episode) + \", score: \" + str(step) + \", episode_loss: \" + str(episode_loss/step))\n",
    "                episode_runs.append(step)\n",
    "                episode_losses.append(episode_loss/step)\n",
    "                epsilon = epsilon * epsilon_decay\n",
    "                epsilon = min(epsilon, epsilon_end)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = dqn_solver.distill(n_bins=[6,6,6,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997590d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill2(self, n_bins=[4,4,4,4], lims=[[-1.2, 1.2], [-2, 2], [-0.2094395, 0.2094395], [-3, 3]]):\n",
    "    self.\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
