{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lnn import Model, Predicate, Variable, And, Or, Predicates, Fact, Loss, Propositions, Implies, World, Variables\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "import math\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "Observation = collections.namedtuple(\"Observation\", (\"Position\", \"Velocity\", \"Angle\", \"AngVelocity\"))\n",
    "Transition = collections.namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class LNNCartpole():\n",
    "    def __init__(self, num_nodes, n_pos, n_vel, n_ang, n_angvel):\n",
    "        def create_predicates(n_nodes, name):\n",
    "            predicate_list = []\n",
    "            for i in range(n_nodes):\n",
    "                predicate_list.append(Predicate(name + str(i+1)))\n",
    "                predicate_list.append(Predicate(name + str(-(i+1))))\n",
    "            return predicate_list\n",
    "\n",
    "        def create_n_ary_and(num_nodes, preds):\n",
    "            and_list = []\n",
    "            for _ in range(num_nodes):\n",
    "                and_list.append(And(*preds[\"Position\"], *preds[\"Velocity\"], *preds[\"Angle\"], *preds[\"AngVelocity\"]))\n",
    "            return and_list\n",
    "\n",
    "        def create_n_ary_or(and_list):\n",
    "            return Or(*and_list)\n",
    "\n",
    "        self.model = Model()\n",
    "        self.preds = {\n",
    "          \"Position\": create_predicates(n_pos, \"pos\"),\n",
    "          \"Velocity\": create_predicates(n_vel, \"vel\"),\n",
    "          \"Angle\": create_predicates(n_ang, \"ang\"),\n",
    "          \"AngVelocity\": create_predicates(n_angvel, \"angvel\")\n",
    "        }\n",
    "        self.and_nodes = create_n_ary_and(num_nodes, self.preds)\n",
    "        self.or_node = create_n_ary_or(self.and_nodes)\n",
    "\n",
    "        self.model.add_knowledge(*self.and_nodes, self.or_node)\n",
    "    def generate_state_dictionary(self, processed_fol):\n",
    "        d = []\n",
    "        for key in self.preds:\n",
    "            arr = [{0: Fact.FALSE}]*(len(self.preds[key])*2)\n",
    "\n",
    "            positive, value = processed_fol[key]\n",
    "\n",
    "            index = 2*(value-1) if positive else 2*value-1\n",
    "            arr[index][0] = Fact.TRUE\n",
    "            d.append(dict(zip(self.preds[key], arr)))\n",
    "        res = {**d[0], **d[1], **d[2], **d[3]}\n",
    "        return res\n",
    "    \n",
    "    def forward(self, processed_fol_arr):\n",
    "        for fol in processed_fol_arr:\n",
    "            state_dict = self.generate_state_dictionary(fol)\n",
    "            \n",
    "            # DETERMINE HOW MODEL PROCESSES FORWARD\n",
    "            # self.model.add_data(state_dict)\n",
    "\n",
    "class FOLCartpoleAgent():\n",
    "    MAXLEN = 10_000\n",
    "    MIN_REPLAY_SIZE = 1_000\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.9\n",
    "    LR = 0.01\n",
    "\n",
    "    def __init__(self, n_bin_args, n_nodes, limits):\n",
    "        self.left_lnn = LNNCartpole(n_nodes, **n_bin_args)\n",
    "        self.right_lnn = LNNCartpole(n_nodes, **n_bin_args)\n",
    "        self.limits = limits\n",
    "        self.bin_args = n_bin_args\n",
    "        self.bin_sizes = {}\n",
    "        for (key1, key2) in zip(self.limits, self.bin_args):\n",
    "            self.bin_sizes[key1] = self.limits[key1][1]/self.bin_args[key2]\n",
    "\n",
    "        self.replay_memory = collections.deque([], maxlen = self.MAXLEN)\n",
    "        \n",
    "    def envs2fol(self, obs_arr):\n",
    "        ret = []\n",
    "        for obs in abs_arr:\n",
    "            ret.append(self.env2fol(obs))\n",
    "        return ret\n",
    "\n",
    "    def env2fol(self, obs):\n",
    "        assert obs.shape == (4,)\n",
    "        obs = Observation(*obs)\n",
    "        ret = {}\n",
    "        for key in self.limits:\n",
    "            val = getattr(obs, key)\n",
    "            positive = (val >= 0)\n",
    "            if positive:\n",
    "                val_bin = math.ceil(val/self.bin_sizes[key])\n",
    "            if val/self.bin_sizes[key] - int(val/self.bin_sizes[key]) == 0:\n",
    "                val_bin += 1\n",
    "                val_bin = min(val_bin, self.bin_args[key])\n",
    "            else:\n",
    "                val_bin = math.floor(val/self.bin_sizes[key])\n",
    "                val_bin = max(val_bin, -self.bin_args[key])\n",
    "\n",
    "            ret[key] = (positive, abs(val_bin))\n",
    "        return ret\n",
    "\n",
    "    def remember(self, obs):\n",
    "        '''\n",
    "          obs: namedtuple given by (state, action, reward, next_state, done)\n",
    "        '''\n",
    "        self.replay_memory.append(obs)\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.replay_memory) < self.MIN_REPLAY_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = [self.replay_memory[idx] for idx in np.random.permutation(len(self.replay_memory))[:self.MINIBATCH_SIZE]]\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = self.envs2fol(np.array(batch.state))\n",
    "\n",
    "        action_batch = torch.tensor(batch.action, device = self.device, dtype = torch.int64)\n",
    "        reward_batch = torch.tensor(batch.reward, device = self.device)\n",
    "        \n",
    "        self.right_lnn.forward(state_batch)\n",
    "        self.left_lnn.forward(state_batch)\n",
    "        \n",
    "        final_mask = torch.tensor([val == False for val in batch.done], device = self.device)\n",
    "        next_state_batch = self.envs2fol(np.array(batch.next_state)[mask])\n",
    "        \n",
    "        label_mask = torch.tensor\n",
    "        \n",
    "\n",
    "    def sample_random_action(self):\n",
    "        return np.random.randint(2)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_fol = [self.env2fol(state)]\n",
    "        \n",
    "        self.right_lnn.forward(state_fol)\n",
    "        self.left_lnn.forward(state_fol)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "WARMUP_STEPS = 1_000\n",
    "NUM_EPISODES = 10_000\n",
    "\n",
    "n_bin_args = {\n",
    "    \"n_pos\": 10,\n",
    "    \"n_vel\": 10,\n",
    "    \"n_ang\": 10,\n",
    "    \"n_angvel\": 10\n",
    "}\n",
    "\n",
    "limits = {\n",
    "    \"Position\": [-1.2, 1.2],\n",
    "    \"Velocity\": [-2, 2],\n",
    "    \"Angle\": [-0.2094395, 0.2094395],\n",
    "    \"AngVelocity\": [-3, 3]\n",
    "}\n",
    "\n",
    "\n",
    "agent = FOLCartpoleAgent(n_bin_args, n_nodes = 10, limits = limits)\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    epsilon = EPSILON_START\n",
    "    episode_num = 0\n",
    "    episode_runs = []\n",
    "    episode_losses = []\n",
    "    for episode in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "        total_loss, step = 0,0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action = agent.sample_random_action()\n",
    "            else:\n",
    "                action = agent.sample_random_action() #agent.get_action(state)\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            reward = reward if not terminal else 0\n",
    "            agent.remember(Transition(state, action, next_state, reward, terminal))\n",
    "            #loss = agent.optimize()\n",
    "            loss = None\n",
    "            state = next_state\n",
    "\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "                step += 1\n",
    "            \n",
    "            if terminal or truncated:\n",
    "                if step > 0:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(step) + \", episode_loss: \" + str(total_loss/step))\n",
    "                    episode_runs.append(step)\n",
    "                    episode_losses.append(total_loss/step)\n",
    "                    epsilon = epsilon * epsilon_decay\n",
    "                    epsilon = min(epsilon, epsilon_end)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "k = agent.replay_memory\n",
    "\n",
    "transitions = [k[idx] for idx in np.random.permutation(len(k))[:10]]\n",
    "batch = Transition(*zip(*transitions))\n",
    "for obs in batch.done:\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([val == False for val in batch.done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████▎                                                               | 3495/10000 [00:02<00:05, 1219.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [132], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [131], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(NUM_EPISODES)):\n\u001b[0;32m      7\u001b[0m     total_loss, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m epsilon:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\time_limit.py:68\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:207\u001b[0m, in \u001b[0;36mCartPoleEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FOLCartpoleAgent at 0x2bbbd923970>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train():\n",
    "\t\tepsilon = EPS_START\n",
    "\t\tepisode_num = 0\n",
    "\t\tstep = 0\n",
    "\t\tfor ep in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "\t\t\t\ttotal_loss, step = 0, 0\n",
    "\t\t\t\tfor query in database.queries:\n",
    "\t\t\t\t\t\tenv.reset(query)\n",
    "\t\t\t\t\t\twhile True:\n",
    "\t\t\t\t\t\t\t\tif env.current_length == MAX_REGEX_LENGTH-1:\n",
    "\t\t\t\t\t\t\t\t\taction = vocab.word_dict[\"$\"]\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\tif np.random.random() > epsilon:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = agent.get_action(query, env.current_state, env.current_length)\n",
    "\t\t\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = env.sample_random_action()\n",
    "\n",
    "\t\t\t\t\t\t\t\tcurr_state = copy.deepcopy(env.current_state)\n",
    "\n",
    "\t\t\t\t\t\t\t\tnext_state, reward, length, done = env.step(action)\n",
    "\t\t\t\t\t\t\t\tif next_state is not None:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor(next_state)\n",
    "\t\t\t\t\t\t\t\t\t\t# if length > 1:\n",
    "\t\t\t\t\t\t\t\t\t\t#     temp_reward = env.hypothesis(curr_state, length)\n",
    "\t\t\t\t\t\t\t\t\t\t#     agent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor(query), torch.tensor(curr_state), torch.tensor(length+1), torch.tensor(vocab.word_dict[\"$\"]),\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor([-1]*10), torch.tensor(temp_reward), torch.tensor(True)))\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor([-1]*10)\n",
    "\t\t\t\t\t\t\t\tagent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\ttorch.tensor(query), torch.tensor(curr_state), torch.tensor(length), torch.tensor(action),\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state, torch.tensor(reward), torch.tensor(done)))\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize()\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\t# if step > 0 and step % 100 == 0:\n",
    "\t\t\t\t\t\t\t\t# \t\tprint(\"step {:d} | loss {:.3f} | lr {:.5f}\".format(\n",
    "\t\t\t\t\t\t\t\t# \t\t\t\tstep, total_loss/step, agent.optimizer.param_groups[0]['lr']))\n",
    "\t\t\t\t\t\t\t\t# \t\tlosses.append(total_loss)\n",
    "\t\t\t\t\t\t\t\t# \t\tsteps.append(step)\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize(only_success=True)\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\tif done:\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t\tepisode_num += 1\n",
    "\t\t\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and episode_num % agent.update_target_every == 0:\n",
    "\t\t\t\t\t\t\t\tprint(\"model updated\")\n",
    "\t\t\t\t\t\t\t\tagent.target_model.load_state_dict(agent.policy_model.state_dict())\n",
    "\t\t\t\tif step > 0:\n",
    "\t\t\t\t\tprint(\"ep {:d} | loss {:.3f}\".format(ep, total_loss/step))\n",
    "\t\t\t\t# Decay epsilon\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and epsilon > EPS_END:\n",
    "\t\t\t\t\t\tepsilon *= EPS_DECAY\n",
    "\t\t\t\t\t\tepsilon = max(EPS_END, epsilon)\n",
    "\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE:\n",
    "\t\t\t\t\t\tif ep % 10 == 0:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = True)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = False)\n",
    "    #torch.save(agent.policy_model.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "062d995d26e67c4332b6a2768f2e0b009e9a42fa9ff300fd46ce3432b7193fb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
