{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lnn import Model, Predicate, Variable, And, Or, Predicates, Fact, Loss, Propositions, Implies, World, Variables\n",
    "import numpy as np\t\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import math\n",
    "\n",
    "Observation = collections.namedtuple(\"Observation\", (\"Position\", \"Velocity\", \"Angle\", \"AngVelocity\"))\n",
    "Transition = collections.namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "class LNNCartpole():\n",
    "    def __init__(self, num_nodes, n_pos, n_vel, n_ang, n_angvel, left):\n",
    "        def create_predicates(n_nodes, name, var):\n",
    "            predicate_list = []\n",
    "            for i in range(n_nodes):\n",
    "                predicate_list.append(Predicate(name + str(i+1))(var))\n",
    "                predicate_list.append(Predicate(name + str(-(i+1)))(var))\n",
    "            return predicate_list\n",
    "\n",
    "        def create_n_ary_and(num_nodes, preds):\n",
    "            and_list = []\n",
    "            for _ in range(num_nodes):\n",
    "                and_list.append(And(*preds[\"Position\"], *preds[\"Velocity\"], *preds[\"Angle\"], *preds[\"AngVelocity\"]))\n",
    "            return and_list\n",
    "\n",
    "        def create_n_ary_or(and_list):\n",
    "            return Or(*and_list)\n",
    "\n",
    "        self.model = Model()\n",
    "        x = Variable('x')\n",
    "        self.preds = {\n",
    "            \"Position\": create_predicates(n_pos, \"pos\", x),\n",
    "            \"Velocity\": create_predicates(n_vel, \"vel\", x),\n",
    "            \"Angle\": create_predicates(n_ang, \"ang\", x),\n",
    "            \"AngVelocity\": create_predicates(n_angvel, \"angvel\", x)\n",
    "        }\n",
    "        self.and_nodes = create_n_ary_and(num_nodes, self.preds)\n",
    "        self.or_node = create_n_ary_or(self.and_nodes)\n",
    "\n",
    "        self.model.add_knowledge(*self.and_nodes, self.or_node)\n",
    "        \n",
    "        self.left = left\n",
    "\n",
    "    def generate_state_dictionary(self, processed_fol_arr):\n",
    "        d = []\n",
    "        for key in self.preds:\n",
    "            value_array = []\n",
    "            for i, fol in enumerate(processed_fol_arr):\n",
    "                positive, value = fol[key]\n",
    "                if self.left:\n",
    "                    positive = not(positive)\n",
    "                index = 2*(value-1) if positive else 2*value-1\n",
    "                for j in range(len(self.preds[key])):\n",
    "                    if len(value_array) <= j:\n",
    "                        value_array.append({})\n",
    "                    \n",
    "                    if j == index:\n",
    "                        value_array[j][str(i)] = Fact.TRUE\n",
    "                    else:\n",
    "                        value_array[j][str(i)] = Fact.FALSE\n",
    "                \n",
    "                predicate_array = np.array(self.preds[key], dtype = object)[:, 0]\n",
    "                \n",
    "                d.append(dict(zip(predicate_array, value_array)))\n",
    "        res = {**d[0], **d[1], **d[2], **d[3]}\n",
    "        return res\n",
    "\n",
    "    def generate_label_dictionary(self, qval_arr, err=0.1):\n",
    "        '''\n",
    "            params:\n",
    "                qval_arr: array of qvals for training\n",
    "                err: float error radius on truth bounds\n",
    "\n",
    "            returns:\n",
    "                label_dict: dictionary {str(i): (qval-err, qval+err)} for each qval in qval_arr\n",
    "        '''\n",
    "        label_dict = {self.or_node: {str(i): (max(qval-err, 0.), min(qval+err, 1.)) for i, qval in enumerate(qval_arr)}}\n",
    "        return label_dict\n",
    "\n",
    "    def forward(self, processed_fol_arr):\n",
    "        '''\n",
    "            params:\n",
    "                processed_fol_arr: array of fol observations used to generate state dict\n",
    "\n",
    "            returns:\n",
    "                output: bsz x 2 tensor of lower/upper bounds for each batch example\n",
    "        '''\n",
    "        self.model.flush()\n",
    "        state_dict = self.generate_state_dictionary(processed_fol_arr)\n",
    "        self.model.add_data(state_dict)\n",
    "        self.model.infer()\n",
    "        return self.or_node.get_data()\n",
    "\n",
    "    def train_step(self, obs, labels, steps=1):\n",
    "        '''\n",
    "            params:\n",
    "                obs: array of dictionaries corresponding to first order logic of input nodes\n",
    "                labels: array of floats corresponding to the labels of observations\n",
    "\n",
    "            returns:\n",
    "                loss: loss over training\t\n",
    "        '''\n",
    "        assert len(obs) == labels.shape(0)\n",
    "\n",
    "        self.model.flush()\n",
    "\n",
    "        state_dict = self.generate_state_dictionary(obs)\n",
    "        self.model.add_data(state_dict)\n",
    "        label_dict = self.generate_label_dictionary(labels)\n",
    "        self.model.add_labels(label_dict)\n",
    "        epochs, loss = self.model.train(losses=[Loss.SUPERVISED], epochs=steps)\n",
    "        return loss\n",
    "\n",
    "class FOLCartpoleAgent():\n",
    "    MAXLEN = 10_000\n",
    "    MIN_REPLAY_SIZE = 1_000\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.9\n",
    "    LR = 0.01\n",
    "\n",
    "    def __init__(self, n_bin_args, n_nodes, limits):\n",
    "        self.left_lnn = LNNCartpole(n_nodes, **n_bin_args, left = True)\n",
    "        self.right_lnn = LNNCartpole(n_nodes, **n_bin_args, left = False)\n",
    "        self.limits = limits\n",
    "        self.bin_args = n_bin_args\n",
    "        self.bin_sizes = {}\n",
    "        for (key1, key2) in zip(self.limits, self.bin_args):\n",
    "            self.bin_sizes[key1] = self.limits[key1][1]/self.bin_args[key2]\n",
    "\n",
    "        self.replay_memory = collections.deque([], maxlen = self.MAXLEN)\n",
    "\n",
    "    def envs2fol(self, obs_arr):\n",
    "        ret = []\n",
    "        for obs in obs_arr:\n",
    "            ret.append(self.env2fol(obs))\n",
    "        return ret\n",
    "\n",
    "    def env2fol(self, obs):\n",
    "        assert obs.shape == (4,)\n",
    "        obs = Observation(*obs)\n",
    "        ret = {}\n",
    "        for (key1, key2) in zip(self.limits, self.bin_args):\n",
    "            val = getattr(obs, key1)\n",
    "            positive = (val >= 0)\n",
    "            if positive:\n",
    "                val_bin = math.ceil(val/self.bin_sizes[key1])\n",
    "                if val/self.bin_sizes[key1] - int(val/self.bin_sizes[key1]) == 0:\n",
    "                    val_bin += 1\n",
    "                val_bin = min(val_bin, self.bin_args[key2])\n",
    "            else:\n",
    "                val_bin = math.floor(val/self.bin_sizes[key1])\n",
    "                val_bin = max(val_bin, -self.bin_args[key2])\n",
    "\n",
    "            ret[key1] = (positive, abs(val_bin))\n",
    "        return ret\n",
    "\n",
    "    def remember(self, obs):\n",
    "        '''\n",
    "            obs: namedtuple given by (state, action, reward, next_state, done)\n",
    "        '''\n",
    "        self.replay_memory.append(obs)\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.replay_memory) < self.MIN_REPLAY_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = [self.replay_memory[idx] for idx in np.random.permutation(len(self.replay_memory))[:self.MINIBATCH_SIZE]]\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        #action_batch = torch.tensor(batch.action, device = self.device, dtype = torch.int64)\n",
    "        reward_batch = torch.tensor(batch.reward, device = self.device)\n",
    "\n",
    "        final_mask = torch.tensor([val == False for val in batch.done], device = self.device)\t\t\n",
    "\n",
    "        next_state_batch = self.envs2fol(np.array(batch.next_state)[final_mask])\n",
    "\n",
    "        right_next_values = self.right_lnn.forward(next_state_batch).mean(dim=1)\n",
    "        left_next_values = self.left_lnn.forward(next_state_batch).mean(dim=1)\n",
    "\n",
    "        next_state_values = torch.zeros(self.MINIBATCH_SIZE, device = self.device)\n",
    "        next_state_values[final_mask] = torch.cat((right_next_values, left_next_values), dim=1).max(dim=1)\n",
    "\n",
    "        expected_next_state_values = next_state_values * self.GAMMA + reward_batch\n",
    "\n",
    "        left_mask = torch.tensor([val == 0 for val in batch.action], device = self.device) #True is left, False is Right\n",
    "        right_mask = left_mask == False\n",
    "\n",
    "        state_batch_right = self.envs2fol(np.array(batch.state)[right_mask])\n",
    "        state_batch_left = self.envs2fol(np.array(batch.state)[left_mask])\n",
    "\n",
    "        loss_left = self.left_lnn.train_step(state_batch_left, expected_next_state_values[left_mask])\n",
    "        loss_right = self.right_lnn.train_step(state_batch_right, expected_next_state_values[right_mask])\n",
    "\n",
    "        return loss_left + loss_right\n",
    "\n",
    "\n",
    "    def sample_random_action(self):\n",
    "        '''\n",
    "            0: left\n",
    "            1: right\n",
    "        '''\n",
    "        return np.random.randint(2)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_fol = [self.env2fol(state)]\n",
    "        left_q = self.left_lnn.forward(state_fol).mean(dim=1)\n",
    "        right_q = self.right_lnn.forward(state_fol).mean(dim=1)\n",
    "        return torch.argmax(torch.cat((left_q, right_q), dim = 0))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "p1, p2 = Predicates('p1', 'p2')\n",
    "x = Variable('x')\n",
    "\n",
    "a1 = And(p1(x), p2(x))\n",
    "\n",
    "model.add_knowledge(a1)\n",
    "\n",
    "model.add_data({\n",
    "    p1: {\n",
    "        '0': Fact.TRUE,\n",
    "        '1': Fact.FALSE\n",
    "    },\n",
    "    p2: {\n",
    "        '0': Fact.TRUE,\n",
    "        '1': Fact.TRUE\n",
    "    }\n",
    "})\n",
    "\n",
    "model.add_labels({\n",
    "    a1: {\n",
    "        '0': Fact.TRUE,\n",
    "        '1': Fact.TRUE\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************************************************************\n",
      "                                LNN Model\n",
      "\n",
      "OPEN And: (p1(0) ∧ p2(0)) \n",
      "params  α: 1.0,  β: 1.0,  w: [0. 1.]\n",
      "'0'                                                         TRUE (1.0, 1.0)\n",
      "'1'                                                         TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Predicate: p2 \n",
      "params  α: 1.0\n",
      "'0'                                                         TRUE (1.0, 1.0)\n",
      "'1'                                                         TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN Predicate: p1 \n",
      "params  α: 1.0\n",
      "'0'                                                         TRUE (1.0, 1.0)\n",
      "'1'                                                        FALSE (0.0, 0.0)\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "model.train(losses = [Loss.SUPERVISED])\n",
    "model.print(params = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "WARMUP_STEPS = 1_000\n",
    "NUM_EPISODES = 10_000\n",
    "\n",
    "n_bin_args = {\n",
    "    \"n_pos\": 10,\n",
    "    \"n_vel\": 5,\n",
    "    \"n_ang\": 10,\n",
    "    \"n_angvel\": 5\n",
    "}\n",
    "\n",
    "limits = {\n",
    "    \"Position\": [-1.2, 1.2],\n",
    "    \"Velocity\": [-2, 2],\n",
    "    \"Angle\": [-0.2094395, 0.2094395],\n",
    "    \"AngVelocity\": [-3, 3]\n",
    "}\n",
    "\n",
    "\n",
    "agent = FOLCartpoleAgent(n_bin_args, n_nodes = 3, limits = limits)\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "action = agent.get_action(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    epsilon = EPSILON_START\n",
    "    episode_num = 0\n",
    "    episode_runs = []\n",
    "    episode_losses = []\n",
    "    for episode in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "        total_loss, step = 0,0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action = agent.sample_random_action()\n",
    "            else:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            reward = reward if not terminal else 0\n",
    "            agent.remember(Transition(state, action, next_state, reward, terminal))\n",
    "            #loss = agent.optimize()\n",
    "            loss = None\n",
    "            state = next_state\n",
    "\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "                step += 1\n",
    "            \n",
    "            if terminal or truncated:\n",
    "                if step > 0:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(step) + \", episode_loss: \" + str(total_loss/step))\n",
    "                    episode_runs.append(step)\n",
    "                    episode_losses.append(total_loss/step)\n",
    "                    epsilon = epsilon * epsilon_decay\n",
    "                    epsilon = min(epsilon, epsilon_end)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "k = agent.replay_memory\n",
    "\n",
    "transitions = [k[idx] for idx in np.random.permutation(len(k))[:10]]\n",
    "batch = Transition(*zip(*transitions))\n",
    "for obs in batch.done:\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([val == False for val in batch.done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████▎                                                               | 3495/10000 [00:02<00:05, 1219.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [132], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [131], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(NUM_EPISODES)):\n\u001b[0;32m      7\u001b[0m     total_loss, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m epsilon:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\time_limit.py:68\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\wrappers\\env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neurosymb\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:207\u001b[0m, in \u001b[0;36mCartPoleEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FOLCartpoleAgent at 0x2bbbd923970>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train():\n",
    "\t\tepsilon = EPS_START\n",
    "\t\tepisode_num = 0\n",
    "\t\tstep = 0\n",
    "\t\tfor ep in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "\t\t\t\ttotal_loss, step = 0, 0\n",
    "\t\t\t\tfor query in database.queries:\n",
    "\t\t\t\t\t\tenv.reset(query)\n",
    "\t\t\t\t\t\twhile True:\n",
    "\t\t\t\t\t\t\t\tif env.current_length == MAX_REGEX_LENGTH-1:\n",
    "\t\t\t\t\t\t\t\t\taction = vocab.word_dict[\"$\"]\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\tif np.random.random() > epsilon:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = agent.get_action(query, env.current_state, env.current_length)\n",
    "\t\t\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = env.sample_random_action()\n",
    "\n",
    "\t\t\t\t\t\t\t\tcurr_state = copy.deepcopy(env.current_state)\n",
    "\n",
    "\t\t\t\t\t\t\t\tnext_state, reward, length, done = env.step(action)\n",
    "\t\t\t\t\t\t\t\tif next_state is not None:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor(next_state)\n",
    "\t\t\t\t\t\t\t\t\t\t# if length > 1:\n",
    "\t\t\t\t\t\t\t\t\t\t#     temp_reward = env.hypothesis(curr_state, length)\n",
    "\t\t\t\t\t\t\t\t\t\t#     agent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor(query), torch.tensor(curr_state), torch.tensor(length+1), torch.tensor(vocab.word_dict[\"$\"]),\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor([-1]*10), torch.tensor(temp_reward), torch.tensor(True)))\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor([-1]*10)\n",
    "\t\t\t\t\t\t\t\tagent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\ttorch.tensor(query), torch.tensor(curr_state), torch.tensor(length), torch.tensor(action),\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state, torch.tensor(reward), torch.tensor(done)))\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize()\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\t# if step > 0 and step % 100 == 0:\n",
    "\t\t\t\t\t\t\t\t# \t\tprint(\"step {:d} | loss {:.3f} | lr {:.5f}\".format(\n",
    "\t\t\t\t\t\t\t\t# \t\t\t\tstep, total_loss/step, agent.optimizer.param_groups[0]['lr']))\n",
    "\t\t\t\t\t\t\t\t# \t\tlosses.append(total_loss)\n",
    "\t\t\t\t\t\t\t\t# \t\tsteps.append(step)\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize(only_success=True)\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\tif done:\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t\tepisode_num += 1\n",
    "\t\t\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and episode_num % agent.update_target_every == 0:\n",
    "\t\t\t\t\t\t\t\tprint(\"model updated\")\n",
    "\t\t\t\t\t\t\t\tagent.target_model.load_state_dict(agent.policy_model.state_dict())\n",
    "\t\t\t\tif step > 0:\n",
    "\t\t\t\t\tprint(\"ep {:d} | loss {:.3f}\".format(ep, total_loss/step))\n",
    "\t\t\t\t# Decay epsilon\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and epsilon > EPS_END:\n",
    "\t\t\t\t\t\tepsilon *= EPS_DECAY\n",
    "\t\t\t\t\t\tepsilon = max(EPS_END, epsilon)\n",
    "\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE:\n",
    "\t\t\t\t\t\tif ep % 10 == 0:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = True)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = False)\n",
    "    #torch.save(agent.policy_model.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "062d995d26e67c4332b6a2768f2e0b009e9a42fa9ff300fd46ce3432b7193fb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
