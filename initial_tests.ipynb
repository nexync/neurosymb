{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lnn import Model, Predicate, Variable, And, Or, Predicates, Fact, Loss, Propositions, Implies, World, Variables\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "import math\n",
    "\n",
    "Observation = collections.namedtuple(\"Observation\", (\"Position\", \"Velocity\", \"Angle\", \"AngVelocity\"))\n",
    "Transition = collections.namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "class LNNCartpole():\n",
    "    def __init__(self, num_nodes, n_pos, n_vel, n_ang, n_angvel):\n",
    "        def create_predicates(n_nodes, name):\n",
    "            predicate_list = []\n",
    "            for i in range(n_nodes):\n",
    "                predicate_list.append(Predicate(name + str(i+1)))\n",
    "                predicate_list.append(Predicate(name + str(-(i+1))))\n",
    "            return predicate_list\n",
    "\n",
    "        def create_n_ary_and(num_nodes, preds):\n",
    "            and_list = []\n",
    "            for _ in range(num_nodes):\n",
    "                and_list.append(And(*preds[\"Position\"], *preds[\"Velocity\"], *preds[\"Angle\"], *preds[\"AngVelocity\"]))\n",
    "            return and_list\n",
    "\n",
    "        def create_n_ary_or(and_list):\n",
    "            return Or(*and_list)\n",
    "\n",
    "        self.model = Model()\n",
    "        self.preds = {\n",
    "          \"Position\": create_predicates(n_pos, \"pos\"),\n",
    "          \"Velocity\": create_predicates(n_vel, \"vel\"),\n",
    "          \"Angle\": create_predicates(n_ang, \"ang\"),\n",
    "          \"AngVelocity\": create_predicates(n_angvel, \"angvel\")\n",
    "        }\n",
    "        self.and_nodes = create_n_ary_and(num_nodes, self.preds)\n",
    "        self.or_node = create_n_ary_or(self.and_nodes)\n",
    "\n",
    "        self.model.add_knowledge(*self.and_nodes, self.or_node)\n",
    "    def generate_initial_state_dictionary(self, raw_bin_dict):\n",
    "        d = []\n",
    "        for key in self.preds:\n",
    "            arr = [{0: Fact.FALSE}]*(len(self.preds[key])*2)\n",
    "\n",
    "            positive, value = raw_bin_dict[key]\n",
    "\n",
    "            index = 2*(value-1) if positive else 2*value-1\n",
    "            arr[index][0] = Fact.TRUE\n",
    "            d.append(dict(zip(self.preds[key], arr)))\n",
    "        res = {**d[0], **d[1], **d[2], **d[3]}\n",
    "        return res\n",
    "\n",
    "    class FOLCartpoleAgent():\n",
    "        MAXLEN = 10_000\n",
    "        MIN_REPLAY_SIZE = 1_000\n",
    "        BATCH_SIZE = 64\n",
    "        GAMMA = 0.9\n",
    "        LR = 0.01\n",
    "\n",
    "        def __init__(self, n_bin_args, n_nodes, limits):\n",
    "            self.left_lnn = LNNCartpole(n_nodes, **n_bin_args)\n",
    "            self.right_lnn = LNNCartpole(n_nodes, **n_bin_args)\n",
    "            self.limits = limits\n",
    "            self.bin_args = n_bin_args\n",
    "            self.bin_sizes = {}\n",
    "            for (key1, key2) in zip(self.limits, self.bin_args):\n",
    "                self.bin_sizes[key1] = self.limits[key1][1]/self.bin_args[key2]\n",
    "\n",
    "            self.replay_memory = collections.deque([], maxlen = self.MAXLEN)\n",
    "\n",
    "        def env2fol(self, obs):\n",
    "            assert obs.shape == (4,)\n",
    "            obs = Observation(*obs)\n",
    "            ret = {}\n",
    "            for key in self.limits:\n",
    "                val = getattr(obs, key)\n",
    "                positive = (val >= 0)\n",
    "                if positive:\n",
    "                    val_bin = math.ceil(val/self.bin_sizes[key])\n",
    "                if val/self.bin_sizes[key] - int(val/self.bin_sizes[key]) == 0:\n",
    "                    val_bin += 1\n",
    "                    val_bin = min(val_bin, self.bin_args[key])\n",
    "                else:\n",
    "                    val_bin = math.floor(val/self.bin_sizes[key])\n",
    "                    val_bin = max(val_bin, -self.bin_args[key])\n",
    "\n",
    "                ret[key] = (positive, abs(val_bin))\n",
    "            return ret\n",
    "\n",
    "        def remember(self, obs):\n",
    "            '''\n",
    "              obs: namedtuple given by (state, action, reward, next_state, done)\n",
    "            '''\n",
    "            self.replay_memory.append(obs)\n",
    "\n",
    "        def optimize(self):\n",
    "            if len(self.replay_memory) < self.MIN_REPLAY_SIZE:\n",
    "                return\n",
    "\n",
    "            transitions = [self.replay_memory[idx] for idx in np.random.permutation(len(self.replay_memory))[:self.MINIBATCH_SIZE]]\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            batch.state\n",
    "\n",
    "            action_batch = torch.tensor(batch.action, device = self.device, dtype = torch.int64)\n",
    "            reward_batch = torch.tensor(batch.reward, device = self.device)\n",
    "\n",
    "        def sample_random_action():\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "WARMUP_STEPS = 1_000\n",
    "NUM_EPISODES = 10_000\n",
    "\n",
    "n_bin_args = {\n",
    "    \"n_pos\": 10,\n",
    "    \"n_vel\": 10,\n",
    "    \"n_ang\": 10,\n",
    "    \"n_angvel\": 10\n",
    "}\n",
    "\n",
    "limits = {\n",
    "    \"Position\": [-1.2, 1.2],\n",
    "    \"Velocity\": [-2, 2],\n",
    "    \"Angle\": [-0.2094395, 0.2094395],\n",
    "    \"AngVelocity\": [-3, 3]\n",
    "}\n",
    "\n",
    "\n",
    "agent = FOLCartpoleAgent(n_bin_args, n_nodes = 10, limits = limits)\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    epsilon = EPSILON_START\n",
    "    episode_num = 0\n",
    "    for ep in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "        total_loss, step = 0,0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action = agent.sample_random_action()\n",
    "            else:\n",
    "                action = agent.get_action(state)\n",
    "            curr_state = copy.deepcopy(env.current_state)\n",
    "            next_state, reward, terminal, truncated, info = env.step(action)\n",
    "            reward = reward if not terminal else 0\n",
    "            next_state = torch.tensor(np.reshape(state_next, [1, observation_space]))\n",
    "            dqn_solver.remember(Observation(state, action, reward, state_next, terminal))\n",
    "            loss = agent.optimize()\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if loss is not None:\n",
    "                episode_loss += loss\n",
    "                step += 1\n",
    "            \n",
    "            if terminal or truncated:\n",
    "                print(\"Run: \" + str(episode) + \", score: \" + str(step) + \", episode_loss: \" + str(total_loss/step))\n",
    "                episode_runs.append(step)\n",
    "                episode_losses.append(episode_loss/step)\n",
    "                epsilon = epsilon * epsilon_decay\n",
    "                epsilon = min(epsilon, epsilon_end)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FOLCartpoleAgent at 0x2bbbd923970>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train():\n",
    "\t\tepsilon = EPS_START\n",
    "\t\tepisode_num = 0\n",
    "\t\tstep = 0\n",
    "\t\tfor ep in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "\t\t\t\ttotal_loss, step = 0, 0\n",
    "\t\t\t\tfor query in database.queries:\n",
    "\t\t\t\t\t\tenv.reset(query)\n",
    "\t\t\t\t\t\twhile True:\n",
    "\t\t\t\t\t\t\t\tif env.current_length == MAX_REGEX_LENGTH-1:\n",
    "\t\t\t\t\t\t\t\t\taction = vocab.word_dict[\"$\"]\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\tif np.random.random() > epsilon:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = agent.get_action(query, env.current_state, env.current_length)\n",
    "\t\t\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taction = env.sample_random_action()\n",
    "\n",
    "\t\t\t\t\t\t\t\tcurr_state = copy.deepcopy(env.current_state)\n",
    "\n",
    "\t\t\t\t\t\t\t\tnext_state, reward, length, done = env.step(action)\n",
    "\t\t\t\t\t\t\t\tif next_state is not None:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor(next_state)\n",
    "\t\t\t\t\t\t\t\t\t\t# if length > 1:\n",
    "\t\t\t\t\t\t\t\t\t\t#     temp_reward = env.hypothesis(curr_state, length)\n",
    "\t\t\t\t\t\t\t\t\t\t#     agent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor(query), torch.tensor(curr_state), torch.tensor(length+1), torch.tensor(vocab.word_dict[\"$\"]),\n",
    "\t\t\t\t\t\t\t\t\t\t#         torch.tensor([-1]*10), torch.tensor(temp_reward), torch.tensor(True)))\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state = torch.tensor([-1]*10)\n",
    "\t\t\t\t\t\t\t\tagent.update_replay_memory(Transition(\n",
    "\t\t\t\t\t\t\t\t\t\ttorch.tensor(query), torch.tensor(curr_state), torch.tensor(length), torch.tensor(action),\n",
    "\t\t\t\t\t\t\t\t\t\tnext_state, torch.tensor(reward), torch.tensor(done)))\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize()\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\t# if step > 0 and step % 100 == 0:\n",
    "\t\t\t\t\t\t\t\t# \t\tprint(\"step {:d} | loss {:.3f} | lr {:.5f}\".format(\n",
    "\t\t\t\t\t\t\t\t# \t\t\t\tstep, total_loss/step, agent.optimizer.param_groups[0]['lr']))\n",
    "\t\t\t\t\t\t\t\t# \t\tlosses.append(total_loss)\n",
    "\t\t\t\t\t\t\t\t# \t\tsteps.append(step)\n",
    "\t\t\t\t\t\t\t\tloss = agent.optimize(only_success=True)\n",
    "\t\t\t\t\t\t\t\tif loss is not None:\n",
    "\t\t\t\t\t\t\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\t\t\t\t\t\tstep += 1\n",
    "\t\t\t\t\t\t\t\tif done:\n",
    "\t\t\t\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t\tepisode_num += 1\n",
    "\t\t\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and episode_num % agent.update_target_every == 0:\n",
    "\t\t\t\t\t\t\t\tprint(\"model updated\")\n",
    "\t\t\t\t\t\t\t\tagent.target_model.load_state_dict(agent.policy_model.state_dict())\n",
    "\t\t\t\tif step > 0:\n",
    "\t\t\t\t\tprint(\"ep {:d} | loss {:.3f}\".format(ep, total_loss/step))\n",
    "\t\t\t\t# Decay epsilon\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE and epsilon > EPS_END:\n",
    "\t\t\t\t\t\tepsilon *= EPS_DECAY\n",
    "\t\t\t\t\t\tepsilon = max(EPS_END, epsilon)\n",
    "\n",
    "\t\t\t\tif len(agent.replay_memory) > agent.MIN_REPLAY_MEMORY_SIZE:\n",
    "\t\t\t\t\t\tif ep % 10 == 0:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = True)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tprec, rec, error= test(verbose = False)\n",
    "    #torch.save(agent.policy_model.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "062d995d26e67c4332b6a2768f2e0b009e9a42fa9ff300fd46ce3432b7193fb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
